{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See: https://github.com/Tathagatd96/Deep-Autoencoder-using-Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.contrib.layers import fully_connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist=input_data.read_data_sets(\"./MNIST_data/\",one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs=784    #28x28 pixels\n",
    "num_hid1=392\n",
    "num_hid2=196\n",
    "num_hid3=num_hid1\n",
    "num_output=num_inputs\n",
    "lr=0.01\n",
    "actf=tf.nn.relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=tf.placeholder(tf.float32,shape=[None,num_inputs])\n",
    "initializer=tf.variance_scaling_initializer()\n",
    "\n",
    "w1=tf.Variable(initializer([num_inputs,num_hid1]),dtype=tf.float32)\n",
    "w2=tf.Variable(initializer([num_hid1,num_hid2]),dtype=tf.float32)\n",
    "w3=tf.Variable(initializer([num_hid2,num_hid3]),dtype=tf.float32)\n",
    "w4=tf.Variable(initializer([num_hid3,num_output]),dtype=tf.float32)\n",
    "\n",
    "b1=tf.Variable(tf.zeros(num_hid1))\n",
    "b2=tf.Variable(tf.zeros(num_hid2))\n",
    "b3=tf.Variable(tf.zeros(num_hid3))\n",
    "b4=tf.Variable(tf.zeros(num_output))\n",
    "\n",
    "hid_layer1=actf(tf.matmul(X,w1)+b1)\n",
    "hid_layer2=actf(tf.matmul(hid_layer1,w2)+b2)\n",
    "hid_layer3=actf(tf.matmul(hid_layer2,w3)+b3)\n",
    "output_layer=actf(tf.matmul(hid_layer3,w4)+b4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=tf.reduce_mean(tf.square(output_layer-X))\n",
    "optimizer=tf.train.AdamOptimizer(lr)\n",
    "train=optimizer.minimize(loss)\n",
    "init=tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch=5\n",
    "batch_size=150\n",
    "num_test_images=10\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epoch):\n",
    "        \n",
    "        num_batches=mnist.train.num_examples//batch_size\n",
    "        for iteration in range(num_batches):\n",
    "            X_batch,y_batch=mnist.train.next_batch(batch_size)\n",
    "            sess.run(train,feed_dict={X:X_batch})\n",
    "            \n",
    "        train_loss=loss.eval(feed_dict={X:X_batch})\n",
    "        print(\"epoch {} loss {}\".format(epoch,train_loss))\n",
    "        \n",
    "\n",
    "    results=output_layer.eval(feed_dict={X:mnist.test.images[:num_test_images]})\n",
    "    \n",
    "    #Comparing original images with reconstructions\n",
    "    f,a=plt.subplots(2,10,figsize=(20,4))\n",
    "    for i in range(num_test_images):\n",
    "        a[0][i].imshow(np.reshape(mnist.test.images[i],(28,28)))\n",
    "        a[1][i].imshow(np.reshape(results[i],(28,28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedAutoencoder:\n",
    "    def __init__(self, input_dim, num_hidden_layers=3, epoch=100, batch_size=250, learning_rate=0.01):\n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.idim = [None]*num_hidden_layers\n",
    "        self.hdim = [None]*num_hidden_layers\n",
    "        self.hidden = [None]*num_hidden_layers\n",
    "        self.weights = [None]*num_hidden_layers\n",
    "        self.biases = [None]*num_hidden_layers\n",
    "\n",
    "        x = tf.placeholder(dtype=tf.float32, shape=[None, input_dim])\n",
    "        initializer=tf.variance_scaling_initializer()\n",
    "        #initializer=tf.random.normal\n",
    "        output_dim = input_dim\n",
    "        act=tf.nn.relu\n",
    "        \n",
    "        # network construction - [input to hidden]            \n",
    "        for i in range(0, num_hidden_layers):\n",
    "            self.idim[i] = int(input_dim / (2*i)) if i else input_dim \n",
    "            self.hdim[i] = int(input_dim / (2*(i+1))) if i < num_hidden_layers-1 else int(input_dim/2)\n",
    "            print('%s, weights [%d, %d] biases %d' % (\"hidden layer \"+str(i+1) if i else \"input to hidden layer 1\", self.idim[i], self.hdim[i], self.hdim[i]))\n",
    "            self.weights[i] = tf.Variable(initializer([self.idim[i], self.hdim[i]]), dtype=tf.float32, name='weights'+str(i))\n",
    "            self.biases[i] = tf.Variable(tf.zeros([self.hdim[i]]), name='biases'+str(i))\n",
    "            \n",
    "            if i == 0:\n",
    "                self.hidden[i] = act(tf.matmul(x, self.weights[i]) + self.biases[i])\n",
    "            else:\n",
    "                self.hidden[i] = act(tf.matmul(self.hidden[i-1], self.weights[i]) + self.biases[i])\n",
    "        \n",
    "        #output layer\n",
    "        print('output layer, weights [%d, %d] biases %d' % (self.hdim[num_hidden_layers-1], output_dim, output_dim))\n",
    "        self.output_weight = tf.Variable(initializer([self.hdim[num_hidden_layers-1], output_dim]), dtype=tf.float32, name='output_weight')\n",
    "        self.output_bias = tf.Variable(tf.zeros([output_dim]), name='output_bias')\n",
    "        self.output_layer = act(tf.matmul(self.hidden[num_hidden_layers-1], self.output_weight)+self.output_bias)\n",
    "\n",
    "\n",
    "        self.x = x\n",
    "        self.loss = tf.reduce_mean(tf.square(self.output_layer-self.x))\n",
    "        self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "        #self.loss = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(self.x, self.output_layer))))\n",
    "        #self.train_op = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss)\n",
    "        \n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "    def train_dataset(self, dataset):\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for i in range(self.epoch):\n",
    "                batch_num=0\n",
    "                num_batches=mnist.train.num_examples//batch_size\n",
    "                for iteration in range(num_batches):\n",
    "                    X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "                    l, _ = sess.run([self.loss, self.train_op], feed_dict={self.x: X_batch})\n",
    "                    batch_num += 1\n",
    "                \n",
    "                print('epoch {0}: loss = {1}'.format(i, l))\n",
    "                self.saver.save(sess, './model.ckpt')        \n",
    "        \n",
    "    def train(self, data):\n",
    "        features = data\n",
    "        features_placeholder = tf.placeholder(features.dtype, features.shape)\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((features_placeholder))\n",
    "        dataset = dataset.shuffle(buffer_size=100)\n",
    "        dataset = dataset.batch(self.batch_size)\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for i in range(self.epoch):\n",
    "                batch_num=0\n",
    "                iter = dataset.make_initializable_iterator()\n",
    "                sess.run(iter.initializer, feed_dict={features_placeholder: features})\n",
    "                iter_op = iter.get_next()\n",
    "                \n",
    "                while True:\n",
    "                    try:\n",
    "                        batch_data = sess.run(iter_op)\n",
    "                        l, _ = sess.run([self.loss, self.train_op], feed_dict={self.x: batch_data})\n",
    "                        batch_num += 1\n",
    "                    except tf.errors.OutOfRangeError:\n",
    "                        break\n",
    "                \n",
    "                print('epoch {0}: loss = {1}'.format(i, l))\n",
    "                self.saver.save(sess, './model.ckpt')\n",
    "        \n",
    "    def test(self, data):\n",
    "        with tf.Session() as sess:\n",
    "            self.saver.restore(sess, './model.ckpt')\n",
    "            hidden, reconstructed = sess.run([self.hidden[num_hidden_layers-1], self.output_layer], feed_dict={self.x: data})\n",
    "        print('input', data)\n",
    "        print('compressed', hidden)\n",
    "        print('reconstructed', reconstructed)\n",
    "        return reconstructed\n",
    "    \n",
    "    def classify(self, data, labels):\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            self.saver.restore(sess, './model.ckpt')\n",
    "            hidden, reconstructed = sess.run([self.hidden[num_hidden_layers-1], self.output_layer], feed_dict={self.x: data})\n",
    "            reconstructed = reconstructed[0]\n",
    "            # loss = sess.run(self.all_loss, feed_dict={self.x: data})\n",
    "            print('data', np.shape(data))\n",
    "            print('reconstructed', np.shape(reconstructed))\n",
    "            loss = np.sqrt(np.mean(np.square(data - reconstructed), axis=1))\n",
    "            print('loss', np.shape(loss))\n",
    "            horse_indices = np.where(labels == 7)[0]\n",
    "            not_horse_indices = np.where(labels != 7)[0]\n",
    "            horse_loss = np.mean(loss[horse_indices])\n",
    "            not_horse_loss = np.mean(loss[not_horse_indices])\n",
    "            print('horse', horse_loss)\n",
    "            print('not horse', not_horse_loss)\n",
    "            return hidden\n",
    "\n",
    "    def decode(self, encoding):\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            self.saver.restore(sess, './model.ckpt')\n",
    "            reconstructed = sess.run(self.output_layer, feed_dict={self.hidden[num_hidden_layers-1]: encoding})\n",
    "        img = np.reshape(reconstructed, (32, 32))\n",
    "        return img\n",
    "    \n",
    "    def results(self, data):\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            self.saver.restore(sess, './model.ckpt')\n",
    "            results = sess.run(self.output_layer, feed_dict={self.x:data})\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_ae = StackedAutoencoder(784, num_hidden_layers=3, epoch=5, batch_size=150, learning_rate=0.01)\n",
    "s_ae.train_dataset(mnist.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = s_ae.results(mnist.test.images[:num_test_images])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing original images with reconstructions\n",
    "f,a=plt.subplots(2,10,figsize=(20,4))\n",
    "for i in range(num_test_images):\n",
    "    a[0][i].imshow(np.reshape(mnist.test.images[i],(28,28)))\n",
    "    a[1][i].imshow(np.reshape(results[i],(28,28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0b1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
